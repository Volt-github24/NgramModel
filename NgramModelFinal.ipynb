{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Volt-github24/NgramModel/blob/main/NgramModelFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2jVWnJbbvWw",
        "outputId": "2a30ee70-f1ac-4717-e1ec-55f87a189a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxL5EryQcNb_",
        "outputId": "4c8e2fac-b3b7-4147-e6fd-fcf3805ed0f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Collab_Stage_L3\n",
            "'Copie de Rapport sur les NLG.gdoc'\n",
            " corpus.txt\n",
            "' Generation de langage lstm and ngram good.ipynb'\n",
            "'Generation de langage naturel corpus francais avec textgnrnn.ipynb'\n",
            " NgramModel.ipynb\n",
            " Prise_en_main_Spacy.ipynb\n",
            "'Rapport d'\\''implementation du Modele Ngram pour la generation des phrases, basé sur un corpus francais (conversation dans une banque).gdoc'\n",
            "\"Rapport d'implementation du tutoriel sur les NLG.gdoc\"\n",
            "'Test Ngram corpus francais.ipynb'\n",
            " textgenrnn_weights.hdf5\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "import os\n",
        "os.chdir(\"/content/drive/\")\n",
        "%cd MyDrive/Collab_Stage_L3/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es3pYwugcVVr",
        "outputId": "75be2108-0073-4afb-f580-e7908b28688f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-17 13:40:22.951266: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 333 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Obligatoire\"\"\"\n",
        "import spacy # importation de la librairie\n",
        "!python -m spacy download fr_core_news_sm # pipeline pour le francais\n",
        "\n",
        "# Fonction de tokenisation\n",
        "def return_token(sentence):\n",
        "\n",
        "  nlp = spacy.load(\"fr_core_news_sm\")\n",
        "  # Tokeniser la phrase\n",
        "  doc = nlp(sentence)\n",
        "  # Retourner le texte de chaque token\n",
        "  return [X.text for X in doc]\n",
        "\n",
        "# Extraction du corpus avec ponctuation\n",
        "# liste = []\n",
        "with open(\"corpus.txt\", \"r\", encoding='latin-1') as f:\n",
        "  for line in f.readlines():\n",
        "    liste.append(line)\n",
        "\n",
        "# Tokenisation du corpus\n",
        "corpus_with_punc = [] # corpus est la liste des tokens\n",
        "for line in liste:\n",
        "  result = return_token(line)[:-1]\n",
        "  corpus_with_punc.append(result)\n",
        "\n",
        "# Enlever la ponctuation dans le corpus\n",
        "punc = [':', \"!\", \"^\", '.', '`', '~', ';', ',', '?', '...', '_', ' ']\n",
        "corpus = []\n",
        "for sentence in corpus_with_punc:\n",
        "  liste_tokens = []\n",
        "  for token in sentence:\n",
        "    if token not in punc :\n",
        "      liste_tokens.append(token)\n",
        "  corpus.append(liste_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aT0HEGlOAnd3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "import spacy # importation de la librairie\n",
        "#!python -m spacy download fr_core_news_sm # pipeline pour le francais\n",
        "\n",
        "class NgramModel():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "      self.word_to_index_unigram = {}\n",
        "      self.index_to_word_unigram = []\n",
        "      self.word_to_index_ngram = {}\n",
        "      self.index_to_word_ngram = []\n",
        "      self.matrix = np.ones((10, 10), dtype = float)\n",
        "      self.ngram_size = 10\n",
        "\n",
        "\n",
        "    # Fonction de tokenisation\n",
        "    def return_token(self, sentence):\n",
        "      print(\"tokenise ...\")  \n",
        "      nlp = spacy.load(\"fr_core_news_sm\")\n",
        "      # Tokeniser la phrase\n",
        "      doc = nlp(sentence)\n",
        "      # Retourner le texte de chaque token\n",
        "      return [X.text for X in doc]\n",
        "\n",
        "\n",
        "    # La fonction qui me permet de creer la liste des sequences de mots et le dictionaire  seequence de mot : index\n",
        "    def create_word_index_ngram(self, corpus, ngram_size = 10): # La fonction prend dorenavant un parametre precisant la taille des grams (initialisee à 3)\n",
        "      print(\"create word to index ngram ... \")\n",
        "      ngram_size = self.ngram_size\n",
        "      first_chaine = \"\" # Comme il faut creer une chaine de START de taille ngramsize, je l'initialise\n",
        "      for _ in range(ngram_size):\n",
        "        first_chaine = first_chaine + \"START \"\n",
        "      first_chaine = first_chaine[:-1] # j'enleve le dernier caractere qui est l'espace, cest la chaine START START START...  qui a l'id 0\n",
        "      word_to_index = {\n",
        "      first_chaine:0,\n",
        "      \"END\":1 # et END a l'id 1\n",
        "      }\n",
        "      index_to_word = [first_chaine, 'END'] # J'initialise la liste des mots\n",
        "\n",
        "      for speech in corpus: # je parcours chaque phrase de mon corpus\n",
        "          for i in range(len(speech)): # pour chaque mot de la phrse, non pas mot mais token\n",
        "              word = \"\" # variable qui va contenir la sequence des mots, la sequence a la taille de ngramsize\n",
        "              if i < ngram_size -1 : # Dans cette condition je constitue la sequence, et ici specifiquement elle commence par START\n",
        "                  word = 'START ' * (ngram_size - i - 1)\n",
        "                  for k in range(i+1): # Je concatene donc le debut des start la avec les tokens suivants\n",
        "                      word = word + speech[k] + ' '\n",
        "              else: # Et ici je constitue les sequences qui ne commencent pas par START, genre ce sont les mots du milieu de la phrase, pas du debut\n",
        "                  for k in range(i-(ngram_size-1), i+1):\n",
        "                      word = word + speech[k] + ' '\n",
        "\n",
        "              word = word[:-1] # j'enleve l'espace de la fin\n",
        "              if word not in index_to_word: # si la sequence n'est pas encore dans la liste je vais l'ajouter, a la derniere ligne\n",
        "                  word_to_index[word] = len(index_to_word) # et je l'ajoute aussi dans le dictionnaire avec son id est son emplacement  dans la liste des sequences\n",
        "                  index_to_word.append(word)\n",
        "              \n",
        "      return word_to_index, index_to_word # Je retourne le dictionnaire et la liste...  \n",
        "\n",
        "\n",
        "    # cette fonction me permet de definir la matrice contenant sequence et mot suivant, les sequences etant de taille ngram_size\n",
        "    def create_ngram_matrix(self, word_to_index, index_to_word, word_to_index_unigram, corpus, ngram_size = 10):\n",
        "      print(\" create ngram matrix ...\")\n",
        "      ngram_size = self.ngram_size\n",
        "      V = len(index_to_word) \n",
        "      V_unigram = len(word_to_index_unigram)\n",
        "      matrix = np.zeros((V, V_unigram)) # Je definis ma matrice, qui est de taille (nombre de sequence de mots * nombre de mots unigrams)\n",
        "\n",
        "      first_chaine = \"\" # Je fabrique la meme premiere chaine la que je vais ajouter a la colonne 0 de la matrice\n",
        "      for _ in range(ngram_size):\n",
        "        first_chaine = first_chaine + \"START \"\n",
        "      first_chaine = first_chaine[:-1]\n",
        "\n",
        "      for sentence in corpus:\n",
        "          for i in range(len(sentence)): # Je parcours donc chaque token de la liste des tokens de la phrase en question\n",
        "              if i < ngram_size-1: # Dans cette condition je verifie que le token actuel est parmi les premiers tokens de la phrase, cest a dire pour les cas ou il n'a pas tous les token precedent s pour avoir la sequence complete à ettre dans la matrice\n",
        "                  taille_start = \"\"\n",
        "                  if i == 0: # a chaque fois je vais mettre la chaine START START ... dans la premiere colonne de la matrice\n",
        "                      matrix[word_to_index[first_chaine], word_to_index_unigram[sentence[i]]] += 1 # et mettre cette case à 1 pour marquer le debut d'une nouvelle phrase\n",
        "                  if len(sentence) > i+1: # si la phrse est asseez longue, je mets la suite des sequences (ici ce sont les sequences qui commencent par START )\n",
        "                      # je fabrique la chaine qui commence par START à faire correspondre en ligne dans la matrice puis je concatene avec les premiers mots et je fais la correspondance dans la matrice cad chercher son successeur\n",
        "                      taille_start = \"START \" * (ngram_size-i-1) \n",
        "                      for j in range(i+1):\n",
        "                          taille_start = taille_start + sentence[j] + \" \"\n",
        "                      matrix[word_to_index[taille_start[:-1]], word_to_index_unigram[sentence[i+1]]] += 1  # j'incremente donc la case correspondant a la sequence et au successeur\n",
        "\n",
        "              elif i >= (len(sentence)-1): # Il s'agit ici du cas ou je suis au dernier caractere de la phrase\n",
        "                  # je fabrique la chaine dont je cherche a mettre le successeur à END et la taille de la chaine est en fonction de la taille du ngram\n",
        "                  taille_chaine = \"\"\n",
        "                  for l in range(i-(ngram_size-1), i+1):\n",
        "                      taille_chaine = taille_chaine + sentence[l] + \" \"\n",
        "                  matrix[word_to_index[taille_chaine[:-1]], 1] += 1 # je met la colonne de END à 1\n",
        "              else: # Il s'agit donc du cas où je suis au milieur de la phrase, quand je tombe sur un mot (ou token), je prends simplement ses precedents, autant qu'il en faut pour faire correspondre a la taille de ngramsize\n",
        "                  taille_chaine = \"\" \n",
        "                  for l in range(i-(ngram_size-1), i+1): # Je concatene ce mot et ses precedents\n",
        "                      taille_chaine = taille_chaine + sentence[l] + \" \"\n",
        "                  matrix[word_to_index[taille_chaine[:-1]], word_to_index_unigram[sentence[i+1]]] += 1 # Puis j'increment la case correspondant e avec le sucesseur en colonne \n",
        "      \n",
        "      return matrix    # Je retourne ensuite la matrice, toute faite\n",
        "      \n",
        "    # Cette fonction permet de constituer la liste des mots (sequence de un seul mot), et le dictionnaire correspondant mot : index\n",
        "    def create_word_index_unigram(self, corpus):\n",
        "     \n",
        "      word_to_index = {\n",
        "      \"START\":0,\n",
        "      \"END\":1\n",
        "      }\n",
        "      \n",
        "      index_to_word = ['START', 'END']\n",
        "\n",
        "      for speech in corpus:\n",
        "          for word in speech: # je parcours chaque phrase  je parcours ses tokens\n",
        "              if word not in index_to_word: # et je n''ajoute un token que s'il n'est pas deja dans la liste des tokens \n",
        "                  word_to_index[word] = len(index_to_word) # et dans le dictionnaire, la valeur pour la cle qui est le mot, est simplement a position de ce mot dans la liste des tokens\n",
        "                  #print(len(index_to_word))\n",
        "                  index_to_word.append(word) # ensuite j'ajoute ce token\n",
        "                  \n",
        "      return word_to_index, index_to_word # et je retourne le dictionnaire et la liste\n",
        "    \n",
        "\n",
        "    # Cette fonction prend en entree l'index d'une sequenc de mot pour retourner l'index de son successeur\n",
        "    def get_next_word(self, index_previous_ngram, matrix, show_nb_possibilities=False): # il prend l'index du mot et retourne l'index du mot suivant\n",
        "      \n",
        "      '''\n",
        "      index_previous_ngram : int, represent the word index in the word_to_index matrix\n",
        "      matrix : successor matrix, lines represent the n-gram, columns represent the successor\n",
        "\n",
        "      1. Check how many successors are possible\n",
        "      2. Select a random word from the possible words, to avoid repetition\n",
        "      '''\n",
        "      \n",
        "      if (matrix[index_previous_ngram]>0).sum() < 2:\n",
        "          nb_possible = 1\n",
        "      elif (matrix[index_previous_ngram]>0).sum() > 9:\n",
        "          nb_possible = 10\n",
        "      else:\n",
        "          nb_possible = (matrix[index_previous_ngram]>0).sum()\n",
        "\n",
        "      if show_nb_possibilities==True:\n",
        "          print('NUMBER POSSIBLE ', nb_possible)\n",
        "\n",
        "      top_indexes = matrix[index_previous_ngram].argsort()[-nb_possible:] # constitue la liste des 10 mots suivants les plus succeptibles (avec le plus d'occurence) , 10 etant le nombre de possibiltes, il varie en trre 1 et 10\n",
        "      random_index = math.floor(random.random()*nb_possible) # juste pour choisir aleatoirement  l'index du prochin mot parmi toutes les possibilites\n",
        "\n",
        "      index_next_word = top_indexes[random_index] # on choisi un index aleatoirement dans cet ensemble des 10 possibilites\n",
        "      \n",
        "      if matrix[index_previous_ngram][index_next_word] == 0: # Si aucun mot n'est successeur, je renvoie le END\n",
        "        index_next_word = 1 # C'est l'indice du END\n",
        "\n",
        "      return index_next_word, matrix[index_previous_ngram][index_next_word]  # retourne l'index du prochain mot, et le nombre d'occurence de ce prochain mot sachant le mot precedent dans le corpus\n",
        "\n",
        "\n",
        "    #fonction permettant de retrouver le 10gram de debut de phrase a partir d'un 10gram\n",
        "    def get_begining_sentence(self, index_ngram, matrix):\n",
        "        print(\" get beginning sentence from a part of sentence ...\")\n",
        "        # 1. get the word 10 gram\n",
        "        sentence = self.index_to_word_ngram(index_ngram)\n",
        "        if \"START\" in sentence : # il s'agit d'un 10 gram de debut de phrase\n",
        "          sentence = sentence.split(\" \") # je convertis en liste pour enlever toutes les occurences de START\n",
        "          while 'START' in sentence:\n",
        "            sentence.remove('START')\n",
        "          return ' '.join(sentence) + ' ' # Je reconstitue en chaine pour retourner la chaine bien faite\n",
        "\n",
        "        else: # ici je recupere le 10 gram tel que le successeur est le premier mot de la sequence\n",
        "          # 1. get the word 10 gram\n",
        "          sentence = self.index_to_word_ngram(index_ngram)\n",
        "          successor = sentence.split(\" \")[0]\n",
        "          index_successor = self.word_to_index_unigram(successor)\n",
        "          \n",
        "\n",
        "    # Entrainement du modele (creer la liste des sequences, le dictionnaire des sequences, la matrice sequence successeur)\n",
        "    def train(self, corpus):\n",
        "\n",
        "      print(\" Model training ...\")\n",
        "\n",
        "      #Get word to index and index to word for unigrams\n",
        "      self.word_to_index_unigram, self.index_to_word_unigram = self.create_word_index_unigram(corpus) # je contruis le dictionnaire et la liste des tokens du corpus en fonction de la taille de ngram (taille de la demie phrase maxi 10)\n",
        "\n",
        "      #Get word to index and index to word for ngrams\n",
        "      self.word_to_index_ngram, self.index_to_word_ngram = self.create_word_index_ngram(corpus, self.ngram_size) # Idem matrice equivalente\n",
        "\n",
        "      #Get ngram matrix\n",
        "      self.matrix = self.create_ngram_matrix(self.word_to_index_ngram, self.index_to_word_ngram, self.word_to_index_unigram, corpus, self.ngram_size)\n",
        "\n",
        "      print(\" Model training Success !!! \")\n",
        "\n",
        "    # Fonction pour generer les phrases\n",
        "    def generate_sentence(self, previous_text, nb_words_after=100): # Il prend juste la phrase à completer, et un parametre qui permet de specifier le nombre de mots qu'il veut à la suite\n",
        "\n",
        "      print(\" Text generation ... \\n\") # Design\n",
        "\n",
        "      #1. tokenisation de la phrase en entree\n",
        "      used_words = self.return_token(previous_text)\n",
        "      while ' ' in used_words :\n",
        "          used_words.remove(' ') # J'enleve dans les espaces dans la phrases à completer\n",
        "\n",
        "      #2. Recuperons les n derniers mot de la demie phrase si n<10 et 10 sinon\n",
        "      \n",
        "      last_ngram = \"\" # J'initialise la variable qui va contenir la demie phrase (ou alors la sequence de 10 mots de la demir phrase)\n",
        "\n",
        "      if len(used_words) == 0: # S'il n'entre rien genre une chaine vide, je lui retourne ce message d'erreur\n",
        "          return \"Entrez un texte avec au moins un caractere.\"\n",
        "\n",
        "      elif len(used_words) < 10: # Cas ou la demie phrase a moins de 10 caracteres, je vais concatener le ngram avec les START devant\n",
        "\n",
        "          chaine_start = \"START \" * (self.ngram_size-len(used_words)) # je constitue le debut de la chaine si jamais la phrase a moins de 10 mots\n",
        "\n",
        "          for h in range(len(used_words), 0, -1):\n",
        "            last_ngram = last_ngram + used_words[-h] + \" \" #  Idem je costitue la sequence ...\n",
        "          \n",
        "          last_ngram = chaine_start + last_ngram # je concatene les start avec la phrase entree\n",
        "\n",
        "      else: \n",
        "\n",
        "         for h in range(self.ngram_size, 0, -1):\n",
        "            last_ngram = last_ngram + used_words[-h] + \" \" #  Idem je constitue la sequence ...   \n",
        "      \n",
        "      last_ngram = last_ngram[:-1] # Pour enlever l'espace de la fin\n",
        "\n",
        "      result = previous_text + \" \" # J'initialise la variable qui va contenir la phrase complete a la fin\n",
        "\n",
        "      list_tuple_last_ngram_used_words_result = []\n",
        "      try:\n",
        "          index_last_ngram = self.word_to_index_ngram[last_ngram] # Je recupere donc l'index de ce ngram\n",
        "          list_tuple_last_ngram_used_words_result.append((last_ngram, used_words, result))\n",
        "      except KeyError: # cas ou la sequence de mot ne corresponds pas à un début de phrase, je cherche toutes les combinaisons avec ces mots entrés\n",
        "          all_combinations = itertools.permutations(used_words, len(used_words)) # je fabirque les combinaisons\n",
        "          for one_ngram in all_combinations:#je parcours chacune de ces combinaisons\n",
        "              last_ngram = \"\"\n",
        "              for i in one_ngram:\n",
        "                  last_ngram = last_ngram + i + \" \" # je constitue la sequence avec les mots de la phrase\n",
        "              last_ngram = last_ngram[:-1] # j'enleve l'espace de la fin\n",
        "              if len(used_words) < 10: # dans le cas ou le nombre de mots etait moins de 10, je complete\n",
        "                  chaine_start = \"START \" * (self.ngram_size-len(used_words)) # je constitue le debut de la chaine si jamais la phrase a moins de 10 mots\n",
        "                  chaine_start = chaine_start + last_ngram # je concatene les start avec la phrase entree\n",
        "              if chaine_start in list(self.word_to_index_ngram.keys()):# Si la chaine fait partie d'une sequence dans la matrice constituee, je la conserver\n",
        "                  used_words = last_ngram.split(\" \")\n",
        "                  result = last_ngram + \" \"\n",
        "                  list_tuple_last_ngram_used_words_result.append((chaine_start, used_words, result))\n",
        "              else: # pour dans ce cas verifier si le last_ngram est une element d'un 10gram pas forcement d'un 10gram qui est un debut de phrase\n",
        "                  for line in list(self.word_to_index_ngram.keys()):\n",
        "                      if last_ngram in line:\n",
        "                          used_words = line.split(\" \")\n",
        "                          used_words = [value for value in used_words if value != 'START' and value !=' ']\n",
        "                          result = ' '.join(used_words) + \" \"\n",
        "                          list_tuple_last_ngram_used_words_result.append((line, used_words, result))\n",
        "                      \n",
        "\n",
        "      for last_ngram, used_words, result in list_tuple_last_ngram_used_words_result[:10]:\n",
        "          \n",
        "          if 'START' in last_ngram:\n",
        "            print('oui')\n",
        "          \n",
        "          result_copy = result\n",
        "          used_words_copy = used_words[:]\n",
        "\n",
        "          list_output = [''] # ici donc le resultat sera une liste, de chaque different resultat obtenu, que ce soit pour chaque ngram ou pour chaque ensemble de resultat generee par chque last_ngram\n",
        "      \n",
        "          index_last_ngram = self.word_to_index_ngram[last_ngram] # Je recupere donc l'index de ce ngram\n",
        "\n",
        "          index_last_ngram_copy = index_last_ngram # je fais une copie de l'index de la demie phrase pour recalculer la suite du texte autant de fois plus bas\n",
        "\n",
        "          if len(used_words) >= 10 : # Si la demie phrase a plus de 06 mots, je lui retourne un seul resultat\n",
        "\n",
        "              for i in range(nb_words_after):  # Je boucle au plus le nombre de fois qu'il veut de mots \n",
        "                  \n",
        "                  index_next_word, occurence = self.get_next_word(index_last_ngram, self.matrix) # Je recupere le mot suivant de la derniere sequence\n",
        "\n",
        "                  if(index_next_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "                    used_words.append(self.index_to_word_unigram[index_next_word]) \n",
        "                    result = result + self.index_to_word_unigram[index_next_word] + \" \"\n",
        "                  else:\n",
        "                      break;    \n",
        "\n",
        "                  last_ngram = \"\"\n",
        "                  for h in range(self.ngram_size, 0, -1): # Pour reconstituer la liste de ngram apres avoir trouvé le successeur du précédent\n",
        "                    last_ngram = last_ngram + used_words[-h] + \" \"    \n",
        "                  index_last_ngram = self.word_to_index_ngram[last_ngram[:-1]]\n",
        "\n",
        "              list_output.append(result[:-1]) # Je retourne donc son un resultat\n",
        "          \n",
        "          else : # je veux generer plusieurs sorties si le nombre de mots est moins de 10 caracteres\n",
        "              nombre_de_sortie = random.randint(6,10) # cette variable contient un nombre aleatoire de resultat que je retourne dans ce cas\n",
        "\n",
        "              for _ in range(nombre_de_sortie): # Je boucle donc pour faire plusieurs predictions, pour generer plusieurs resultats comme à la base, le nombre de mot etait petit\n",
        "                  index_last_ngram = index_last_ngram_copy # Je reinitialise donc l'index a la sequence de debut pour refaire la meme operation\n",
        "                  result = result_copy # Je reinitialise la variable resultat\n",
        "                  used_words = used_words_copy[:] # Egalement la liste des tokens de la demie phrase\n",
        "                  while ' ' in used_words : # J'enleve encore tous les espaces dedans\n",
        "                      used_words.remove(' ')\n",
        "                  taille_last_ngram = len(used_words[:])\n",
        "\n",
        "                  for i in range(nb_words_after):  # Je boucle au plus le nombre de fois qu'il veut de mots \n",
        "                  \n",
        "                      index_next_word, occurence = self.get_next_word(index_last_ngram, self.matrix) # Je recupere le mot suivant de la derniere sequence\n",
        "\n",
        "                      if(index_next_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "                        used_words.append(self.index_to_word_unigram[index_next_word])\n",
        "                        result = result + self.index_to_word_unigram[index_next_word] + \" \"\n",
        "                      else:\n",
        "                          break;    \n",
        "\n",
        "                      # Je vais modifier la taille des ngrams et reformuler la matrice afin de considerer pour la prochaine etape plus une sequence de mot plus grand (chose qui ameliore le resultat)\n",
        "                      if taille_last_ngram < 10 : # je m'arrete a 10 car, 10 caracteres precedeents pour trouver le suivant est, on va dire  suffisant!\n",
        "                          \n",
        "                          taille_last_ngram = taille_last_ngram + 1 # j'incremente la taille de la sequene de mot que je prends pour la prochaine etape\n",
        "                          \"\"\" mettre a jour la valeur de last_ngram avec les START START, etc \"\"\"  \n",
        "\n",
        "                      last_ngram = \"\"\n",
        "                      chaine_start = \"START \" * (self.ngram_size-taille_last_ngram) # je constitue le debut de la chaine si jamais la phrase a moins de 10 mots                  \n",
        "                      for h in range(taille_last_ngram, 0, -1):\n",
        "                        last_ngram = last_ngram + used_words[-h] + \" \" #  Idem je constitue la sequence ...\n",
        "                      \n",
        "                      last_ngram = chaine_start + last_ngram # je concatene les start avec la les derniers mots de la phrase\n",
        "\n",
        "                      index_last_ngram = self.word_to_index_ngram[last_ngram[:-1]]\n",
        "                  \n",
        "                  list_output.append(result[:-1]) # J'ajoute un  des resultats sorti\n",
        "                  list_output.append('\\n') # Juste pour armoniser l'affichage à la sortie\n",
        "                \n",
        "      return (list_output) # je retourne la liste en supprimant les doubons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pO1UQXcCG-g",
        "outputId": "810da1d8-6a1c-4a34-c9ec-1161241e16ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model training ...\n",
            "create word to index ngram ... \n",
            " create ngram matrix ...\n",
            " Model training Success !!! \n"
          ]
        }
      ],
      "source": [
        "ngram_model = NgramModel()\n",
        "ngram_model.train(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmX3IH9fCKGl",
        "outputId": "65b360e7-cd1a-4054-ce96-83d4cc173b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Text generation ... \n",
            "\n",
            "tokenise ...\n",
            " m' appelle Ikome Emmanuel Ndumbe Je suis en formation professionnelle chez Camtel Yaounde Cameroun en tant que Technicien Système Informatique et Réseau Je suis ouvert à de nouvelles opportunités s' il y a des disponibilités dans cette Entreprise Je serai ouvert au travail\n"
          ]
        }
      ],
      "source": [
        "print(*ngram_model.generate_sentence(\"professionnelle\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NgramModelFinal.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNf8SIDWvUudpHFd+t+LgDy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}