{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ngrams tests 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzD0FQgGxRyVPvnMpMNCl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Volt-github24/NgramModel/blob/main/ngrams_tests_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4DBNA_qMStM",
        "outputId": "1384f189-51f8-407b-e321-8fa00155334c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "import os\n",
        "os.chdir(\"/content/drive/\")\n",
        "%cd MyDrive/Collab_Stage_L3/\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF1MrCHEUngA",
        "outputId": "a46d726a-2e7a-44c4-8491-ab26837fbc8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Collab_Stage_L3\n",
            "'Copie de Rapport sur les NLG.gdoc'\n",
            " corpus.txt\n",
            "'Generation de langage naturel corpus francais avec textgnrnn.ipynb'\n",
            " Prise_en_main_Spacy.ipynb\n",
            "\"Rapport d'implementation du tutoriel sur les NLG.gdoc\"\n",
            "'Test Ngram corpus francais.ipynb'\n",
            " textgenrnn_weights.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction de tokenisation pour tokeniser le texte d'entree\n"
      ],
      "metadata": {
        "id": "DXppPcIoGHWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "# Fonction de Tokenisation\n",
        "\n",
        "import spacy # importation de la librairie\n",
        "!python -m spacy download fr_core_news_sm # pipeline pour le francais\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "def return_token(sentence):\n",
        "    # Tokeniser la phrase\n",
        "    doc = nlp(sentence)\n",
        "    # Retourner le texte de chaque token\n",
        "    return [X.text for X in doc]    \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXQP8Q2GFfUN",
        "outputId": "e393fa49-9416-4b18-b990-23783a462a9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-06 10:02:51.833734: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenisation du texte  (corpus)"
      ],
      "metadata": {
        "id": "wo2ri7iMGaP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "# Corpus avec ponctuation\n",
        "liste = []\n",
        "with open(\"corpus.txt\", \"r\", encoding='latin-1') as f:\n",
        "  for line in f.readlines():\n",
        "    liste.append(line)\n",
        "\n",
        "# Tokenisation du corpus\n",
        "corpus_with_punc = [] # corpus est la liste des tokens\n",
        "for line in liste:\n",
        "  result = return_token(line)[:-1]\n",
        "  corpus_with_punc.append(result)\n",
        "\n",
        "punc = [':', \"!\", \"^\", '.', '`', '~', ';', ',', '?', '...', '_', ' ']"
      ],
      "metadata": {
        "id": "hamdWXBWGdq5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enlevons la ponctuation dans le corpus"
      ],
      "metadata": {
        "id": "zhu1M4zaUb7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Obligatoire \"\"\"\n",
        "\n",
        "corpus = []\n",
        "for sentence in corpus_with_punc:\n",
        "  liste_tokens = []\n",
        "  for token in sentence:\n",
        "    if token not in punc :\n",
        "      liste_tokens.append(token)\n",
        "  corpus.append(liste_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "oVQ0heSnUgpp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, je teste une nouvelle approche pour les ngrams, ainsi donc, le principe est que pour chaque sequence de mot, il faut savoir ses differents successeurs, cest a dire que quand j'ai une sequence de mots, je dois avoir un tableau qui me montre tous les successeurs possibles (a partir du corpus fournit ) pour cette sequence de mots, apres ce la, comme la machine ne comprend que les nombres, on va constituer un dictionnaire (mot : index) et un dictionnaire (index : mot) pour avoir l'index d'un mot et par la meme occasion le mot correspondant à un index"
      ],
      "metadata": {
        "id": "VfAClG3IsuWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNIGRAMS**"
      ],
      "metadata": {
        "id": "4S1RGMqzm2Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "# UNIGRAMS\n",
        "\n",
        "#Ici il s'agit du cas selon lequel la sequence de mot est un seul mot.\n",
        "#construisons donc les dictionnaires en question  \n",
        "\n",
        "def create_word_index_unigram(liste_tokens):\n",
        "    word_to_index = {\n",
        "    \"START\":0,\n",
        "    \"END\":1\n",
        "    }\n",
        "    \n",
        "    index_to_word = ['START', 'END']\n",
        "\n",
        "    for speech in liste_tokens:\n",
        "        for word in speech: # je parcours chaque phrase  je parcours ses tokens\n",
        "            if word not in index_to_word: # et je n''ajoute un token que s'il n'est pas deja dans la liste des tokens \n",
        "                word_to_index[word] = len(index_to_word) # et dans le dictionnaire, la valeur pour la cle qui est le mot, est simplement a position de ce mot dans la liste des tokens\n",
        "                #print(len(index_to_word))\n",
        "                index_to_word.append(word) # ensuite j'ajoute ce token\n",
        "                \n",
        "    return word_to_index, index_to_word # et je retourne le dictionnaire et la liste\n",
        "\n",
        "# dictionnaire, liste = create_word_index_unigram(liste_tokens)"
      ],
      "metadata": {
        "id": "KftQLYEiqpx9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construisons à présent la matrice mots:successeurs"
      ],
      "metadata": {
        "id": "TP6yyBVkNXf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_unigram_matrix(word_to_index, index_to_word, texts):\n",
        "    V = len(index_to_word) #taille de la matrice, la matrice est de v*v, v etant le nombre de mots differents qu'il ya dans le corpus'\n",
        "    matrix = np.zeros((V, V)) # je cree la matrice en question\n",
        "\n",
        "    for sentence in texts: #je parcours chaque phrase du corpus\n",
        "        for i in range(len(sentence)):\n",
        "            if i == 0:\n",
        "                matrix[0, word_to_index[sentence[i]]] += 1 # Si le mot est le premier d'une phrase, je met la colonne de START à 1, pour marque que cest le debut d'une phrase\n",
        "            else: \n",
        "                matrix[word_to_index[sentence[i-1]], word_to_index[sentence[i]]] += 1 # dans le cas ou il est un mot dans une phrase, j'incremenete la case mot successeur\n",
        "\n",
        "        if i == (len(sentence)-1):\n",
        "            matrix[word_to_index[sentence[i]], 1] += 1 # Si le mot est le dernier d'une phrase, je met la colonne de END à 1, pour marque que cest la fin d'une phrase\n",
        "\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "pUWzkgRKLVcg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Get word to index and index to word for unigrams\n",
        "word_to_index_unigram, index_to_word_unigram = create_word_index_unigram(corpus)\n",
        "\n",
        "#Get unigram matrix\n",
        "unigram_matrix = create_unigram_matrix(word_to_index_unigram,  index_to_word_unigram, corpus)"
      ],
      "metadata": {
        "id": "9pmKNezJS86T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour la suite essayons de generer le texte(completer) avec cette matice construite sur la base de unigrams"
      ],
      "metadata": {
        "id": "nCVHxhmnPPyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction pour retrouver le mot suivant :  il prend l'index du mot et retourne l'index du mot suivant"
      ],
      "metadata": {
        "id": "HVsGnt9Qge_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "def get_next_word(index_previous_ngram, matrix, show_nb_possibilities=False): # il prend l'index du mot et retourne l'index du mot suivant\n",
        "    '''\n",
        "    index_previous_ngram : int, represent the word index in the word_to_index matrix\n",
        "    matrix : successor matrix, lines represent the n-gram, columns represent the successor\n",
        "\n",
        "    1. Check how many successors are possible\n",
        "    2. Select a random word from the possible words, to avoid repetition\n",
        "    '''\n",
        "    \n",
        "    if (matrix[index_previous_ngram]>0).sum() < 2:\n",
        "        nb_possible = 1\n",
        "    elif (matrix[index_previous_ngram]>0).sum() > 9:\n",
        "        nb_possible = 10\n",
        "    else:\n",
        "        nb_possible = (matrix[index_previous_ngram]>0).sum()\n",
        "\n",
        "    if show_nb_possibilities==True:\n",
        "        print('NUMBER POSSIBLE ', nb_possible)\n",
        "\n",
        "    top_indexes = matrix[index_previous_ngram].argsort()[-nb_possible:] # constitue la liste des 10 mots suivants les plus succeptibles (avec le plus d'occurence) , 10 etant le nombre de possibiltes, il varie en trre 1 et 10\n",
        "    random_index = math.floor(random.random()*nb_possible) # juste pour choisir aleatoirement  l'index du prochin mot parmi toutes les possibilites\n",
        "\n",
        "    index_next_word = top_indexes[random_index] # on choisi un index aleatoirement dans cet ensemble des 10 possibilites\n",
        "    \n",
        "    if matrix[index_previous_ngram][index_next_word] == 0: # Si aucun mot n'est successeur, je renvoie le END\n",
        "      index_next_word = 1 # C'est l'indice du END\n",
        "\n",
        "    return index_next_word, matrix[index_previous_ngram][index_next_word]  # retourne l'index du prochain mot, et le nombre d'occurence de ce prochain mot sachant le mot precedent dans le corpus"
      ],
      "metadata": {
        "id": "1TQzWXFnVJtG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction pour compteter le texte avec unigram"
      ],
      "metadata": {
        "id": "19YM0yQAgmnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_unigram(previous_text, word_to_index, index_to_word,  matrix, nb_words_after=100, no_repetition=True):\n",
        "\n",
        "    #1. tokenisation de la phrse en entree\n",
        "    used_words = return_token(previous_text)\n",
        "\n",
        "    #2. Recuperons le dernier mot de la demie phrase\n",
        "    last_word = used_words[-1]\n",
        "\n",
        "    print(previous_text, end=\" \")\n",
        "    index_last_word = word_to_index[last_word] # on recupere l'index du dernier mot de la demie phrase\n",
        "    \n",
        "    for i in range(nb_words_after):      #  cette boucle est la pour que si le break du bas ne s'execute pas, qu'on complete au plus le nombre de mots precisé\n",
        "        index_last_word, occurence = get_next_word(index_last_word, matrix)\n",
        "                \n",
        "        if(index_last_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "          used_words.append(index_to_word[index_last_word])\n",
        "          print(index_to_word[index_last_word], end = ' ') # j'affiche donc au fur et à mesure sans aller à la ligne\n",
        "        else:\n",
        "            break;\n",
        "\n",
        "            \n",
        "\n",
        "#get_sentence_unigram(\"ENEO\", word_to_index_unigram, index_to_word_unigram,unigram_matrix,nb_words_after=10)"
      ],
      "metadata": {
        "id": "eNhmofnSgpq2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AU final, avec les unigrams, le modele fait tellement d'erreurs car il ne tient pas compte du passe lointain de la phrase a completer, essayons donc les bigrams pour ameliorer les resultats\n",
        " Cest a dire on considere pour sequence de mots (deux mots)"
      ],
      "metadata": {
        "id": "0ae11feYlqMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BIGRAMS**"
      ],
      "metadata": {
        "id": "-5KxnLAHmwOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tout d'abord definissons d'abor les dictionnaires comme dans la methode unigram"
      ],
      "metadata": {
        "id": "T0VJjxaYnU4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_index_bigram(texts):\n",
        "    word_to_index = {\n",
        "    \"START START\":0,\n",
        "    \"END\":1\n",
        "    }\n",
        "    index_to_word = ['START START', 'END'] # exactement comme plus haut, sauf qu'ici les sequences sont constituees de deux mots, raison pour laquelle on a START START\n",
        "\n",
        "    for speech in texts:\n",
        "        for i in range(len(speech)):\n",
        "            if i == 0:\n",
        "                word = 'START ' + speech[0] # si je suis avec le premier mot d'une phrse pour stocker cette sequence, je concatene START avec ce premier mot\n",
        "            else:\n",
        "                word = speech[i-1] + ' ' + speech[i] # si cest dans la phrase, je le prend juste avec son precedent\n",
        "\n",
        "            if word not in index_to_word:\n",
        "                word_to_index[word] = len(index_to_word) # j'ajoute dans le dictionnaire \n",
        "                index_to_word.append(word) # et dans la liste , si biensur la sequence n'y etait pas encore\n",
        "            \n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "def create_bigram_matrix(word_to_index, index_to_word, word_to_index_unigram, texts): # elle prend le dictionnaire des bigrams, la liste des bigrams, le dictionnaire des unigram, car les mots suivant sont un sequence d'un seul mot, bien evidement et le texte corpus aussi\n",
        "    V = len(index_to_word)\n",
        "    V_unigram = len(word_to_index_unigram)\n",
        "    matrix = np.zeros((V, V_unigram)) # on cree la matrice de taille , nombre de bigrams, nombre d'unigrams, les unigrams sont les successeurs\n",
        "\n",
        "    for sentence in texts:\n",
        "        for i in range(len(sentence)):\n",
        "            if i == 0: # cas ou je suis avec le premier mot de la phrase\n",
        "                matrix[word_to_index['START START'], word_to_index_unigram[sentence[0]]] += 1            # pour marquer le debut de la phrase\n",
        "                if len(sentence)>1:\n",
        "                  matrix[word_to_index['START '+sentence[0]], word_to_index_unigram[sentence[1]]] += 1      # pour marquer le debut de la phrase\n",
        "\n",
        "            elif i >= (len(sentence)-1):\n",
        "                matrix[word_to_index[sentence[i-1]+' '+sentence[i]], 1] += 1 # si je ne suis pas dans le debut de la phrase, mais au dernier mot plutot je met plutot la colonne de END  a 1 pour marquer la fin de la phrase.\n",
        "       \n",
        "            else: \n",
        "                matrix[word_to_index[sentence[i-1]+' '+sentence[i]], word_to_index_unigram[sentence[i+1]]] += 1 # si je suis au milieu de la phrase, eh bien j'uncrement simplement la case correspondante au successeur\n",
        "    \n",
        "    return matrix"
      ],
      "metadata": {
        "id": "99sKikSvmyEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get word to index and index to word for bigrams\n",
        "word_to_index_bigram, index_to_word_bigram = create_word_index_bigram(corpus)\n",
        "\n",
        "#Get bigram matrix\n",
        "bigram_matrix = create_bigram_matrix(word_to_index_bigram, index_to_word_bigram, word_to_index_unigram, corpus)\n",
        "\n",
        "index_to_word_bigram[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1lW6tB3Mbw3",
        "outputId": "60dc7c90-08e2-440c-9946-134b60446233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['START START',\n",
              " 'END',\n",
              " 'START Bonjour',\n",
              " 'Bonjour Bienvenue',\n",
              " 'Bienvenue sur',\n",
              " 'sur la',\n",
              " 'la plateforme',\n",
              " 'plateforme WhatsApp',\n",
              " 'WhatsApp du',\n",
              " 'du CCA',\n",
              " 'CCA -',\n",
              " '- Bank',\n",
              " 'Bank Vous',\n",
              " 'Vous pouvez',\n",
              " 'pouvez également',\n",
              " 'également nous',\n",
              " 'nous joindre',\n",
              " 'joindre en',\n",
              " 'en appelant',\n",
              " 'appelant notre']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generons maintenant le texte avec le modele bigram"
      ],
      "metadata": {
        "id": "s4kPW1qUBOLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_bigram(previous_text, word_to_index, index_to_word, index_to_word_unigram, matrix, nb_words_after=20, no_repetition=True):\n",
        "    \n",
        "    #1. tokenisation de la phrse en entree\n",
        "    used_words = return_token(previous_text)\n",
        "\n",
        "    #2. Recuperons les deux dernier mot de la demie phrase\n",
        "    if len(used_words)<1: \n",
        "      last_bigram = \"START \" + used_words[-1]\n",
        "    else:\n",
        "      last_bigram = used_words[-2] + \" \" + used_words[-1]  \n",
        "\n",
        "    print(previous_text, end=\" \")\n",
        "\n",
        "    index_last_bigram = word_to_index[last_bigram]\n",
        "\n",
        "    for i in range(nb_words_after):  \n",
        "        \n",
        "        index_next_word, occurence = get_next_word(index_last_bigram, matrix)\n",
        "                \n",
        "        if(index_next_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "          used_words.append(index_to_word_unigram[index_next_word])\n",
        "          print(index_to_word_unigram[index_next_word], end = ' ') # j'affiche donc au fur et à mesure sans aller à la ligne\n",
        "        else:\n",
        "            break;\n",
        "\n",
        "        index_last_bigram = word_to_index[used_words[len(used_words)-2] + ' ' + used_words[len(used_words)-1]] # juste pour mettre a jour la variable contenant l'id du dernier bigram, celle qu'on repasse a la fonction permettant d'avoir le successeur probable\n",
        "            "
      ],
      "metadata": {
        "id": "iXnpWqZkwlMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx, occurence = get_next_word(word_to_index_bigram['suis pas'], bigram_matrix)\n",
        "print(index_to_word_unigram[idx])\n",
        "print(occurence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxZhynzE1Vzk",
        "outputId": "a2e0eb5c-d61d-4d7d-e080-b82e89b4c87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encore\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentence_bigram(\"solde (espace)\", \n",
        "                    word_to_index_bigram, \n",
        "                    index_to_word_bigram, \n",
        "                    index_to_word_unigram, \n",
        "                    bigram_matrix,\n",
        "                   nb_words_after=100\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbdrvin811kr",
        "outputId": "c74c64c7-f35b-49b5-9669-5fba7bc96779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "solde (espace) votre ID au 8171 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'efficacite des bigrams peut etre augmentee en passant aux trigrams, testons ce modele aussi\n",
        " Cest a dire on considere pour sequence de mots (trois mots)"
      ],
      "metadata": {
        "id": "j4nvUe0f3KIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRIGRAMS**"
      ],
      "metadata": {
        "id": "CjaJNVP03KIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tout d'abord definissons d'abor les dictionnaires comme dans la methode bigram"
      ],
      "metadata": {
        "id": "YxdC4b3N3KIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_word_index_trigram(texts):\n",
        "    word_to_index = {\n",
        "    \"START START START\":0,\n",
        "    \"END\":1\n",
        "    }\n",
        "    index_to_word = ['START START START', 'END']\n",
        "\n",
        "    for speech in texts:\n",
        "        for i in range(len(speech)):\n",
        "            if i == 0:\n",
        "                word = 'START START ' + speech[i]\n",
        "            elif i==1:\n",
        "                word = 'START ' + speech[i-1] + ' ' + speech[i]\n",
        "            else:\n",
        "                word = speech[i-2] + ' ' + speech[i-1] + ' ' + speech[i]\n",
        "\n",
        "            #print(word)\n",
        "            if word not in index_to_word:\n",
        "                word_to_index[word] = len(index_to_word)\n",
        "                index_to_word.append(word)\n",
        "            \n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "def create_trigram_matrix(word_to_index, index_to_word, word_to_index_unigram, texts):\n",
        "    V = len(index_to_word)\n",
        "    V_unigram = len(word_to_index_unigram)\n",
        "    matrix = np.zeros((V, V_unigram))\n",
        "\n",
        "    for sentence in texts:\n",
        "        for i in range(len(sentence)):\n",
        "            if i == 0:\n",
        "                matrix[word_to_index['START START START'], word_to_index_unigram[sentence[0]]] += 1 \n",
        "                if len(sentence)>1:                \n",
        "                  matrix[word_to_index['START START '+sentence[0]], word_to_index_unigram[sentence[i+1]]] += 1 \n",
        "\n",
        "            elif i ==1:\n",
        "              if len(sentence)>2:\n",
        "                matrix[word_to_index['START '+sentence[i-1] + ' ' +sentence[i]], word_to_index_unigram[sentence[i+1]]] += 1\n",
        "\n",
        "            elif i >= (len(sentence)-1):\n",
        "                matrix[word_to_index[sentence[i-2] +' '+ sentence[i-1]+' '+sentence[i]], 1] += 1\n",
        "       \n",
        "            else: \n",
        "                matrix[word_to_index[sentence[i-2]+' '+sentence[i-1]+' '+sentence[i]], word_to_index_unigram[sentence[i+1]]] += 1\n",
        "    \n",
        "    return matrix"
      ],
      "metadata": {
        "id": "a65i4dej3KIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get word to index and index to word for trigrams\n",
        "word_to_index_trigram, index_to_word_trigram = create_word_index_trigram(corpus)\n",
        "\n",
        "#Get trigram matrix\n",
        "trigram_matrix = create_trigram_matrix(word_to_index_trigram, index_to_word_trigram, word_to_index_unigram, corpus)\n",
        "\n",
        "index_to_word_trigram[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIVv8L0U3KIr",
        "outputId": "4a0fc388-1e6c-4b31-ba62-32af95a642a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['START START START',\n",
              " 'END',\n",
              " 'START START Bonjour',\n",
              " 'START Bonjour Bienvenue',\n",
              " 'Bonjour Bienvenue sur',\n",
              " 'Bienvenue sur la',\n",
              " 'sur la plateforme',\n",
              " 'la plateforme WhatsApp',\n",
              " 'plateforme WhatsApp du',\n",
              " 'WhatsApp du CCA',\n",
              " 'du CCA -',\n",
              " 'CCA - Bank',\n",
              " '- Bank Vous',\n",
              " 'Bank Vous pouvez',\n",
              " 'Vous pouvez également',\n",
              " 'pouvez également nous',\n",
              " 'également nous joindre',\n",
              " 'nous joindre en',\n",
              " 'joindre en appelant',\n",
              " 'en appelant notre']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generons maintenant le texte avec le modele trigram"
      ],
      "metadata": {
        "id": "Ud4iSJrc3KIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_trigram(previous_text, word_to_index, index_to_word, index_to_word_unigram, matrix, nb_words_after=20, no_repetition=True):\n",
        "\n",
        "    #1. tokenisation de la phrse en entree\n",
        "    used_words = return_token(previous_text)\n",
        "\n",
        "    #2. Recuperons les trois derniers mot de la demie phrase\n",
        "    if len(used_words)<2: \n",
        "      last_trigram = \"START \" + used_words[-1]\n",
        "    elif len(used_words)<3:\n",
        "      last_trigram = \"START \" + used_words[-2] + \" \" + used_words[-1]  \n",
        "    else:\n",
        "      last_trigram = used_words[-3] + \" \" + used_words[-2] + \" \" + used_words[-1] \n",
        "    \n",
        "    print(previous_text, end=\" \")\n",
        "\n",
        "    index_last_trigram = word_to_index[last_trigram]\n",
        "\n",
        "    for i in range(nb_words_after):  \n",
        "        \n",
        "        index_next_word, occurence = get_next_word(index_last_trigram, matrix)\n",
        "\n",
        "        if(index_next_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "          used_words.append(index_to_word_unigram[index_next_word])\n",
        "          print(index_to_word_unigram[index_next_word], end = ' ') # j'affiche donc au fur et à mesure sans aller à la ligne\n",
        "        else:\n",
        "            break;    \n",
        "        \n",
        "        index_last_trigram = word_to_index[used_words[len(used_words)-3] + ' ' + used_words[len(used_words)-2] + ' ' + used_words[len(used_words)-1]]\n",
        "            "
      ],
      "metadata": {
        "id": "zEvJXOLX3KIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentence_trigram(\"Hors l'établissement d'une\", \n",
        "                    word_to_index_trigram, \n",
        "                    index_to_word_trigram, \n",
        "                    index_to_word_unigram, \n",
        "                    trigram_matrix,\n",
        "                   nb_words_after=20\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b3c0d5-e1f7-4b40-f8ab-d589c88cb5a9",
        "id": "AnPyXfVC3KIv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hors l'établissement d'une carte bancaire bien vouloir vous rapprocher de la délégation du ministère des finances "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant essayons d'ecrire un algorithme qui prendra en parametre la taille n des grams"
      ],
      "metadata": {
        "id": "BUfA7v5K_CuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N GRAMS**"
      ],
      "metadata": {
        "id": "Tr4ESkUDRDx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation des deux dictionnaire et liste"
      ],
      "metadata": {
        "id": "CTri2kl27nh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "def create_word_index_ngram(texts, ngram_size = 3): # La fonction prend dorenavant un parametre precisant la taille des grams (initialisee à 3)\n",
        "    first_chaine = \"\" # Comme il faut creer une chaine de START de taille ngramsize, je l'initialise\n",
        "    for _ in range(ngram_size):\n",
        "      first_chaine = first_chaine + \"START \"\n",
        "    first_chaine = first_chaine[:-1] # j'enleve le dernier caractere qui est l'espace, cest la chaine START START START...  qui a l'id 0\n",
        "    word_to_index = {\n",
        "    first_chaine:0,\n",
        "    \"END\":1 # et END a l'id 1\n",
        "    }\n",
        "    index_to_word = [first_chaine, 'END'] # J'initialise la liste des mots\n",
        "\n",
        "    for speech in texts: # je parcours chaque phrase de mon corpus\n",
        "        for i in range(len(speech)): # pour chaque mot de la phrse, non pas mot mais token\n",
        "            word = \"\" # variable qui va contenir la sequence des mots, la sequence a la taille de ngramsize\n",
        "            if i < ngram_size -1 : # Dans cette condition je constitue la sequence, et ici specifiquement elle commence par START\n",
        "                word = 'START ' * (ngram_size - i - 1)\n",
        "                for k in range(i+1): # Je concatene donc le debut des start la avec les tokens suivants\n",
        "                    word = word + speech[k] + ' '\n",
        "            else: # Et ici je constitue les sequences qui ne commencent pas par START, genre ce sont les mots du milieu de la phrase, pas du debut\n",
        "                for k in range(i-(ngram_size-1), i+1):\n",
        "                    word = word + speech[k] + ' '\n",
        "\n",
        "            word = word[:-1] # j'enleve l'espace de la fin\n",
        "            if word not in index_to_word: # si la sequence n'est pas encore dans la liste je vais l'ajouter, a la derniere ligne\n",
        "                word_to_index[word] = len(index_to_word) # et je l'ajoute aussi dans le dictionnaire avec son id est son emplacement  dans la liste des sequences\n",
        "                index_to_word.append(word)\n",
        "            \n",
        "    return word_to_index, index_to_word # Je retourne le dictionnaire et la liste..."
      ],
      "metadata": {
        "id": "ZGhhf9wFRKsv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation de la matrice n-gram"
      ],
      "metadata": {
        "id": "7EHrleK974bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "# Cette fonction aussi prend le ngramsize en parametre et cela doit etre le meme que celui qui a ete utilisé dans create_word_index_ngram\n",
        "def create_ngram_matrix(word_to_index, index_to_word, word_to_index_unigram, texts, ngram_size = 3):\n",
        "    V = len(index_to_word) \n",
        "    V_unigram = len(word_to_index_unigram)\n",
        "    matrix = np.zeros((V, V_unigram)) # Je definis ma matrice, qui est de taille (nombre de sequence de mots * nombre de mots unigrams)\n",
        "\n",
        "    first_chaine = \"\" # Je fabrique la meme premiere chaine la que je vais ajouter a la colonne 0 de la matrice\n",
        "    for _ in range(ngram_size):\n",
        "      first_chaine = first_chaine + \"START \"\n",
        "    first_chaine = first_chaine[:-1]\n",
        "\n",
        "    for sentence in texts:\n",
        "        for i in range(len(sentence)): # Je parcours donc chaque token de la liste des tokens de la phrase en question\n",
        "            if i < ngram_size-1: # Dans cette condition je verifie que le token actuel est parmi les premiers tokens de la phrase, cest a dire pour les cas ou il n'a pas tous les token precedent s pour avoir la sequence complete à ettre dans la matrice\n",
        "                taille_start = \"\"\n",
        "                if i == 0: # a chaque fois je vais mettre la chaine START START ... dans la premiere colonne de la matrice\n",
        "                    matrix[word_to_index[first_chaine], word_to_index_unigram[sentence[i]]] += 1 # et mettre cette case à 1 pour marquer le debut d'une nouvelle phrase\n",
        "                if len(sentence) > i+1: # si la phrse est asseez longue, je mets la suite des sequences (ici ce sont les sequences qui commencent par START )\n",
        "                    # je fabrique la chaine qui commence par START à faire correspondre en ligne dans la matrice puis je concatene avec les premiers mots et je fais la correspondance dans la matrice cad chercher son successeur\n",
        "                    taille_start = \"START \" * (ngram_size-i-1) \n",
        "                    for j in range(i+1):\n",
        "                        taille_start = taille_start + sentence[j] + \" \"\n",
        "                    matrix[word_to_index[taille_start[:-1]], word_to_index_unigram[sentence[i+1]]] += 1  # j'incremente donc la case correspondant a la sequence et au successeur\n",
        "\n",
        "            elif i >= (len(sentence)-1): # Il s'agit ici du cas ou je suis au dernier caractere de la phrase\n",
        "                # je fabrique la chaine dont je cherche a mettre le successeur à END et la taille de la chaine est en fonction de la taille du ngram\n",
        "                taille_chaine = \"\"\n",
        "                for l in range(i-(ngram_size-1), i+1):\n",
        "                    taille_chaine = taille_chaine + sentence[l] + \" \"\n",
        "                matrix[word_to_index[taille_chaine[:-1]], 1] += 1 # je met la colonne de END à 1\n",
        "            else: # Il s'agit donc du cas où je suis au milieur de la phrase, quand je tombe sur un mot (ou token), je prends simplement ses precedents, autant qu'il en faut pour faire correspondre a la taille de ngramsize\n",
        "                taille_chaine = \"\" \n",
        "                for l in range(i-(ngram_size-1), i+1): # Je concatene ce mot et ses precedents\n",
        "                    taille_chaine = taille_chaine + sentence[l] + \" \"\n",
        "                matrix[word_to_index[taille_chaine[:-1]], word_to_index_unigram[sentence[i+1]]] += 1 # Puis j'increment la case correspondant e avec le sucesseur en colonne \n",
        "    \n",
        "    return matrix    # Je retourne ensuite la matrice, toute faite"
      ],
      "metadata": {
        "id": "ClKluXP8aY6m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appelons maintenant ces fonctions pour constituer notre matrice (le noeud de la chose)"
      ],
      "metadata": {
        "id": "1DesNDbdXwhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get word to index and index to word for trigrams\n",
        "word_to_index_ngram, index_to_word_ngram = create_word_index_ngram(corpus, 2)\n",
        "\n",
        "#Get ngram matrix\n",
        "ngram_matrix = create_ngram_matrix(word_to_index_ngram, index_to_word_ngram, word_to_index_unigram, corpus, 2)\n",
        "\n",
        "index_to_word_ngram[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-YuPLWPfPLg",
        "outputId": "29dcfd50-f0e4-4786-c700-8a8df902a99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['START START',\n",
              " 'END',\n",
              " 'START Bonjour',\n",
              " 'Bonjour Bienvenue',\n",
              " 'Bienvenue sur',\n",
              " 'sur la',\n",
              " 'la plateforme',\n",
              " 'plateforme WhatsApp',\n",
              " 'WhatsApp du',\n",
              " 'du CCA',\n",
              " 'CCA -',\n",
              " '- Bank',\n",
              " 'Bank Vous',\n",
              " 'Vous pouvez',\n",
              " 'pouvez également',\n",
              " 'également nous',\n",
              " 'nous joindre',\n",
              " 'joindre en',\n",
              " 'en appelant',\n",
              " 'appelant notre']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme la methode de detection du mot suivant est deja fonctionnelle, implementons la fonction permettant de generer la suite de la phrase"
      ],
      "metadata": {
        "id": "nVhm_ELJf2J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "def get_sentence_ngram(previous_text, nb_words_after=100): # Il prend juste la phrase à completer, et un parametre qui permet de specifier le nombre de mots qu'il veut à la suite\n",
        "\n",
        "    print(\" Generation text ... \\n\") # Design\n",
        "\n",
        "    #1. tokenisation de la phrase en entree\n",
        "    used_words = return_token(previous_text)\n",
        "    while ' ' in used_words :\n",
        "        used_words.remove(' ') # J'enleve dans les espaces dans la phrases à completer\n",
        "\n",
        "    #2. Recuperons les n derniers mot de la demie phrase si n<10 et n sinon\n",
        "    \n",
        "    last_ngram = \"\" # J'initialise la variable qui va contenir la demie phrase (ou alors la sequence de 10 mots de la demir phrase)\n",
        "\n",
        "    if len(used_words) == 0: # S'il n'entre rien genre une chaine vide, je lui retourne ce message d'erreur\n",
        "        return \"Entrez un texte avec au moins un caractere.\"\n",
        "\n",
        "    elif len(used_words) < 10: # Cas ou la demie phrase a plus de 10 caracteres, je considere ngram comme une sequence de len(used_words)\n",
        "      ngram_size = len(used_words) # la taille des sequences à former est simplement la taille de la demie phrase entrée\n",
        "      \n",
        "      for h in range(ngram_size, 0, -1):\n",
        "          last_ngram = last_ngram + used_words[-h] + \" \" # Je constitue donc la la sequence (de fin de la phrase entree, si pas toute la phrase entree)\n",
        "\n",
        "    else : # Cas ou la demie phrase a plus de 10 caracteres, je considere ngram comme une sequence de 10\n",
        "      \n",
        "      ngram_size = 10 # la taille des sequences à former est simplement 10\n",
        "      for h in range(ngram_size, 0, -1):\n",
        "          last_ngram = last_ngram + used_words[-h] + \" \" #  Idem je costitue la sequence ...\n",
        "\n",
        "    ngram_size_copy = ngram_size # je fais une copie de la taille de sequence de mots car dans le cas ou cest inferieur à 6, je vais proposer plusieurs sorties et donc pour les prochaines sorties , j'aurais besoin de recommencer à considerer les sequences de taille ngrams avant d'incrementer jusqua 6 encore\n",
        "\n",
        "    # 3. Fabriquons maintenant les dictionnaires, la liste des sequences et la matrice en fonction de la taille des ngrams, \n",
        "\n",
        "    #Get word to index and index to word for unigrams\n",
        "    word_to_index_unigram, index_to_word_unigram = create_word_index_unigram(corpus) # je contruis le dictionnaire et la liste des tokens du corpus en fonction de la taille de ngram (taille de la demie phrase maxi 10)\n",
        "\n",
        "    #Get word to index and index to word for ngrams\n",
        "    word_to_index_ngram, index_to_word_ngram = create_word_index_ngram(corpus, ngram_size) # Idem matrice equivalente\n",
        "\n",
        "    #Get ngram matrix\n",
        "    matrix = create_ngram_matrix(word_to_index_ngram, index_to_word_ngram, word_to_index_unigram, corpus, ngram_size)\n",
        "    \n",
        "    last_ngram = last_ngram[:-1] # Pour enlever l'espace de la fin\n",
        "\n",
        "    result = previous_text + \" \" # J'initialise la variable qui va contenir la phrase complete a la fin\n",
        "    \n",
        "    index_last_ngram = word_to_index_ngram[last_ngram] # Je recupere donc l'index de ce ngram\n",
        "\n",
        "    index_last_ngram_copy = index_last_ngram # je fais une copie de l'index de la demie phrase pour recalculer la suite du texte autant de fois plus bas\n",
        "\n",
        "    if ngram_size >= 6 : # SI la demie phrase a plus de 06 mots, je lui retourne un seul resultat\n",
        "        for i in range(nb_words_after):  # Je boucle au plus le nombre de fois qu'il veut de mots \n",
        "            \n",
        "            index_next_word, occurence = get_next_word(index_last_ngram, matrix) # Je recupere le mot suivant de la derniere sequence\n",
        "\n",
        "            if(index_next_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "              used_words.append(index_to_word_unigram[index_next_word]) \n",
        "              result = result + index_to_word_unigram[index_next_word] + \" \"\n",
        "            else:\n",
        "                break;    \n",
        "\n",
        "            last_ngram = \"\"\n",
        "            for h in range(ngram_size, 0, -1): # Pour reconstituer la liste de ngram apres avoir trouvé le successeur du précédent\n",
        "              last_ngram = last_ngram + used_words[-h] + \" \"    \n",
        "            index_last_ngram = word_to_index_ngram[last_ngram[:-1]]\n",
        "\n",
        "        return result[:-1] # Je retourne donc son un resultat\n",
        "    \n",
        "    else : # je veux generer plusieurs sorties si le nombre de mots est moins de 06 caracteres\n",
        "        set_of_results = [] # ici donc le resultatt sera une liste, de chaque different resultat obtenu\n",
        "        nombre_de_sortie = random.randint(6,10) # cette variable contient un nombre aleatoire de resultat que je retourne dans ce cas\n",
        "\n",
        "        \"\"\" J'ameliore ca cette nuit \"\"\"\n",
        "\n",
        "        for _ in range(nombre_de_sortie): # Je boucle donc pour faire plusieurs predictions, pour generer plusieurs resultats comme à la base, le nombre de mot etait petit\n",
        "            index_last_ngram = index_last_ngram_copy # Je reinitialise donc l'index a la sequence de debut pour refaire la meme operation\n",
        "            result = previous_text + \" \" # Je reinitialise la variable resultat\n",
        "            ngram_size = ngram_size_copy # je reprends le processus pour la prochaine sortie proposee en considerant le ngramsize de depart\n",
        "            used_words = return_token(previous_text) # Egalement la liste des tokens de la demie phrase\n",
        "            while ' ' in used_words : # J'enleve encore tous les espaces dedans\n",
        "                used_words.remove(' ')\n",
        "\n",
        "            # Comme j'ai modifié la matrice en bas, je dois la restituer car les seequences de mots dans cette matrice etaient de taille 6, et en reprenant le dernier last ngram, la sequence est de taille ce qu'il a entré , donc je reconstitue la matrice\n",
        "\n",
        "            #Get word to index and index to word for ngrams\n",
        "            word_to_index_ngram, index_to_word_ngram = create_word_index_ngram(corpus, ngram_size) # Idem matrice equivalente\n",
        "\n",
        "            #Get ngram matrix\n",
        "            matrix = create_ngram_matrix(word_to_index_ngram, index_to_word_ngram, word_to_index_unigram, corpus, ngram_size)\n",
        "\n",
        "            for i in range(nb_words_after):  # Je boucle au plus le nombre de fois qu'il veut de mots \n",
        "            \n",
        "                index_next_word, occurence = get_next_word(index_last_ngram, matrix) # Je recupere le mot suivant de la derniere sequence\n",
        "\n",
        "                if(index_next_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "                  used_words.append(index_to_word_unigram[index_next_word])\n",
        "                  result = result + index_to_word_unigram[index_next_word] + \" \"\n",
        "                else:\n",
        "                    break;    \n",
        "\n",
        "                # Je vais modifier la taille des ngrams et reformuler la matrice afin de considerer pour la prochaine etape plus une sequence de mot plus grand (chose qui ameliore le resultat)\n",
        "                if ngram_size < 6 : # je m'arrete a 6 car, 6 caracteres precedeents pour trouver le suivant est, on va dire  suffisant!\n",
        "                    \n",
        "                    ngram_size = ngram_size + 1 # j'incremente la taille de la sequene de mot que je prends pour la prochaine etape\n",
        "\n",
        "                    #Get word to index and index to word for ngrams, i redo it because i changed ngra_size\n",
        "                    word_to_index_ngram, index_to_word_ngram = create_word_index_ngram(corpus, ngram_size) # Idem matrice equivalente\n",
        "\n",
        "                    #Get ngram matrix, i redo it because i changed ngra_size\n",
        "                    matrix = create_ngram_matrix(word_to_index_ngram, index_to_word_ngram, word_to_index_unigram, corpus, ngram_size)\n",
        "\n",
        "                last_ngram = \"\"\n",
        "                for h in range(ngram_size, 0, -1): # Pour reconstituer la liste de ngram apres avoir trouvé le successeur du précédent\n",
        "                  last_ngram = last_ngram + used_words[-h] + \" \"    \n",
        "                index_last_ngram = word_to_index_ngram[last_ngram[:-1]]\n",
        "            \n",
        "            set_of_results.append(result[:-1]) # J'ajoute un  des resultats sorti\n",
        "            set_of_results.append(\" \") # Juste pour armoniser la sortie\n",
        "          \n",
        "        return set_of_results\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "2sv8OZxogGby"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentence_ngram(\" Merci  de  consulter \", 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEokCsouHGM6",
        "outputId": "ffc560c5-0169-40d4-bbe3-9c8278c54411"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generation text ... \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\" Merci  de  consulter  vos mails nous vous avons fait parvenir la fiche de souscription au service C - Online Bien vouloir de l' imprimer la remplir convenablement et nous la renvoyer sur WhatsApp ou par mail sous un format PDF s' il vous plait\",\n",
              " ' ',\n",
              " \" Merci  de  consulter  vos mails et messages spams nous vous avons fais parvenir la fiche de demande de service Bien vouloir l' imprimer la remplir convenablement ( en vous assurant de cocher uniquement C - Online réinitialisation mot de passe ) et nous la renvoyer sur WhatsApp ou par mail accompagnï¿½e de la photocopie de votre CNI ou passeport sur laquelle vous signez 03 fois Tous vos documents sous le format PDF s' il vous plaît\",\n",
              " ' ',\n",
              " \" Merci  de  consulter  vos mails et messages spams nous vous avons fait parvenir le formulaire de souscription au service C - Online bien vouloir l' imprimer le remplir convenablement ( en vous assurant de cocher uniquement le service C - Online dï¿½blocage compte et rï¿½initialisation mot de passe ) et nous la renvoyer sur WhatsApp ou par mail accompagnï¿½e de la photocopie de votre CNI ou passeport sur laquelle vous signez 03 fois Bien vouloir contacter votre gestionnaire MME KEPNIA Annie 696605929\",\n",
              " ' ',\n",
              " \" Merci  de  consulter  vos mails nous vous avons fait parvenir la fiche de demande de service bien vouloir l' imprimer le remplir convenablement(en vous assurant de cocher uniquement le service C - Online déblocage compte et aussi réinitialisation mot de passe ) Accompagnez ce document de votre copie de passeport sur laquelle vous signez 03 fois Tous vos documents en PDF s' il vous plaît\",\n",
              " ' ',\n",
              " \" Merci  de  consulter  vos mails et messages spams nous vous avons fais parvenir la fiche de demande de service Bien vouloir l' imprimer la remplir convenablement ( en vous assurant de cocher uniquement le service C - Online ) et nous le renvoyer sur whatsapp ou par mail accompagnï¿½e de la photocopie de votre CNI ou passeport sur laquelle vous signez 03 fois Tous vos documents doivent être au format PDF s' il vous plaît\",\n",
              " ' ',\n",
              " ' Merci  de  consulter  vos mails nous vous avons fais parvenir le formulaire de souscription au service C - Online ne peut être effectuée',\n",
              " ' ',\n",
              " \" Merci  de  consulter  vos mails nous vous avons fais parvenir la fiche de demande de service bien vouloir l' imprimer le remplir convenablement ( en vous assurant de cocher uniquement le service C - Online ) et nous le renvoyer sur whatsapp ou par mail accompagnï¿½e de la photocopie de votre CNI ou passeport sur laquelle vous signez 03 fois Bien vouloir contacter votre gestionnaire MME NGUELIEUGOUE Epse FOKA 696606668 pour plus d' informations liées à votre virement Le service C online vous facilite la consultation de votre solde en ligne et bien d' autres opérations Vous pouvez souscrire au service en agence\",\n",
              " ' ',\n",
              " \" Merci  de  consulter  vos mails et messages spams nous vous avons fais parvenir le formulaire de souscription au service C - Online Bien vouloir de l' imprimer la remplir convenablement et nous la renvoyer sur WhatsApp ou par mail accompagnée de la photocopie de votre CNI ou passeport sur laquelle vous signez 03 fois Bien vouloir contacter votre gestionnaire MME DJUISSI Arlette 698401039 emailBonjour Bienvenue sur la plateforme WhatsApp du CCA - Bank Vous pouvez ï¿½galement nous joindre en appelant notre service client au 222 51 80 80/679 00 96 30 ou nous laisser un mail à l' adresse support@cca-bank.com Votre satisfaction est\",\n",
              " ' ']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentence_ngram(\"Vous devez souscrire\", 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzsnrkjsUIzP",
        "outputId": "b1dd0444-bb4c-4372-b6c1-eaf58346f130"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generation text ... \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Vous devez souscrire au service C - Online',\n",
              " ' ',\n",
              " 'Vous devez souscrire au service C - Online',\n",
              " ' ',\n",
              " 'Vous devez souscrire au service C - Online et vos paramètres de connexion ( login et mot de passe)vous seront envoyés',\n",
              " ' ',\n",
              " 'Vous devez souscrire au service C - Online et vos paramètres de connexion ( login et mot de passe)vous seront envoyés',\n",
              " ' ',\n",
              " 'Vous devez souscrire au service C - Online et vos paramètres de connexion ( login et mot de passe)vous seront envoyés',\n",
              " ' ',\n",
              " 'Vous devez souscrire au service C - Online et vos paramètres de connexion ( login et mot de passe)vous seront envoyés',\n",
              " ' ',\n",
              " 'Vous devez souscrire au service C - Online et vos paramètres de connexion ( login et mot de passe)vous seront envoyés',\n",
              " ' ',\n",
              " 'Vous devez souscrire au service C - Online et vos paramètres de connexion ( login et mot de passe)vous seront envoyés',\n",
              " ' ',\n",
              " 'Vous devez souscrire au service C - Online et vos paramètres de connexion ( login et mot de passe)vous seront envoyés',\n",
              " ' ',\n",
              " 'Vous devez souscrire au service C - Online et vos paramètres de connexion ( login et mot de passe)vous seront envoyés',\n",
              " ' ']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "A = [10, 5, 'Hi', 10, 5, 'Hi']\n",
        "temp = combinations(A, 5)\n",
        "for i in list(temp):\n",
        "\tprint (i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWF72e4Hh9FR",
        "outputId": "d13024d9-589e-40b0-e232-ed48a554cfa8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 5, 'Hi', 10, 5)\n",
            "(10, 5, 'Hi', 10, 'Hi')\n",
            "(10, 5, 'Hi', 5, 'Hi')\n",
            "(10, 5, 10, 5, 'Hi')\n",
            "(10, 'Hi', 10, 5, 'Hi')\n",
            "(5, 'Hi', 10, 5, 'Hi')\n"
          ]
        }
      ]
    }
  ]
}